{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "from copy import copy\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Code\n",
    "----------------\n",
    "\n",
    "Feel free to skip reading this part, unless you are curious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer:\n",
    "    def __init__(self):\n",
    "        self.mean = 0\n",
    "        self.std = 0\n",
    "\n",
    "    def normalize(self, X, train=True):\n",
    "        \"\"\"normalizes an sample x to have 0 mean and unit standard deviation\"\"\"\n",
    "        if train:\n",
    "            self.mean = np.mean(X)\n",
    "            self.std = np.mean(X)\n",
    "\n",
    "        return((X - self.mean)/self.std)\n",
    "\n",
    "\n",
    "def load_mnist_raw(path, kind='train'):\n",
    "    \"\"\"Load Fashion MNIST data from path\"\"\"\n",
    "    labels_path = os.path.join(path, '%s-labels-idx1-ubyte.gz' % kind)\n",
    "    images_path = os.path.join(path, '%s-images-idx3-ubyte.gz' % kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8, offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "            offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    return(images, labels)\n",
    "\n",
    "\n",
    "def make_mnist_proc(output):\n",
    "\n",
    "    if os.path.isfile(output):\n",
    "        print('preprocessed MNIST already exists, skipping preprocessing')\n",
    "        return(None)\n",
    "\n",
    "    split = 50000\n",
    "    X_train, y_train = load_mnist_raw('data/', kind='train')\n",
    "    X_test, y_test = load_mnist_raw('data/', kind='t10k')\n",
    "\n",
    "    X_valid = X_train[split:, :]\n",
    "    X_train = X_train[:split, :]\n",
    "    y_valid = y_train[split:]\n",
    "    y_train = y_train[:split]\n",
    "\n",
    "    norm_buddy = Normalizer()\n",
    "    X_train = norm_buddy.normalize(X_train)\n",
    "    X_valid = norm_buddy.normalize(X_valid, train=False)\n",
    "    X_test  = norm_buddy.normalize(X_test, train=False)\n",
    "\n",
    "    data = {\"X\": {\"train\": X_train, \"valid\": X_valid, \"test\": X_test},\n",
    "            \"y\": {\"train\": y_train, \"valid\": y_valid, \"test\": y_test}}\n",
    "\n",
    "    print('saving preprocessed MNIST data at {}'.format(output))\n",
    "    with open(output, 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    return(data)\n",
    "\n",
    "\n",
    "def get_circles_data():\n",
    "    data = np.loadtxt(open('data/circles.txt','r'))\n",
    "    X = data[:, :2]\n",
    "    y = data[:, 2]\n",
    "\n",
    "    X_train = X[:800, :]\n",
    "    X_valid = X[800:950, :]\n",
    "    X_test  = X[950:, :]\n",
    "    y_train = y[:800]\n",
    "    y_valid = y[800:950]\n",
    "    y_test  = y[950:]\n",
    "\n",
    "    data = {\"X\": {\"train\": X_train, \"valid\": X_valid, \"test\": X_test},\n",
    "            \"y\": {\"train\": y_train, \"valid\": y_valid, \"test\": y_test}}\n",
    "\n",
    "    return(data)\n",
    "\n",
    "\n",
    "def get_minibatches(n, size):\n",
    "    \"\"\"\n",
    "    gets a bunch of minibatches of the defined size (roughly) out of\n",
    "    the n samples.\n",
    "    \"\"\"  \n",
    "    assert 1 < size < n\n",
    "    \n",
    "    idx = np.arange(n)\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    # In case the minibatch size does not evenly divide into the samples.\n",
    "    rem = n % size\n",
    "    if rem != 0:\n",
    "        idx = np.hstack((idx, np.repeat(-1, (size-rem))))\n",
    "\n",
    "    # Reshape into a list of numpy arrays.\n",
    "    mbs = []\n",
    "    idx = idx.reshape(int(len(idx) / size), size)\n",
    "    for mb in np.arange(idx.shape[0]):\n",
    "        mbs.append(idx[mb, :])\n",
    "\n",
    "    # If -1's are in the final batch, remove them.\n",
    "    mbs[-1] = np.delete(mbs[-1], np.where(mbs[-1] == -1)[0])\n",
    "    if len(mbs[-1]) == 0: \n",
    "        mbs = mbs[:-1]\n",
    "        \n",
    "    return mbs\n",
    "\n",
    "\n",
    "def plot_decision(model, X, y, title=\"\", h=0.07):\n",
    "    \"\"\"plot the decision boundary. h controls plot quality.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "    # https://stackoverflow.com/a/19055059/6027071\n",
    "    # sample a region larger than our training data X\n",
    "    x_min = X[:, 0].min() - 0.5\n",
    "    x_max = X[:, 0].max() + 0.5\n",
    "    y_min = X[:, 1].min() - 0.5\n",
    "    y_max = X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Plot decision boundaries.\n",
    "    tiles_X = np.concatenate(([xx.ravel()], [yy.ravel()]))\n",
    "    tiles_y = np.zeros(tiles_X.shape[1])  \n",
    "    pred, _, _ = model.pass_data(tiles_X.T, tiles_y)\n",
    "    pred = pred.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, pred, alpha=0.8,cmap='RdYlBu')\n",
    "\n",
    "    # Plot points (coloured by class).\n",
    "    ax.scatter(X[:, 0], X[:, 1], alpha=0.8, c=y, cmap='RdYlBu')\n",
    "    ax.axis('off')\n",
    "\n",
    "    ax.set_title(\"Decision Boundary: {}\".format(title))\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_results(results, title):\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "    for i, result_type in enumerate(results.keys()):  \n",
    "        ax = axs.ravel()[i]\n",
    "        ax.plot(results[result_type]['train'], \n",
    "                label='{} {}'.format(result_type, 'train'))\n",
    "        ax.plot(results[result_type]['valid'], \n",
    "                label='{} {}'.format(result_type, 'valid'))\n",
    "        ax.plot(results[result_type]['test'], \n",
    "                label='{} {}'.format(result_type, 'test'))\n",
    "        ax.legend()\n",
    "        ax.set_ylabel(result_type)                                             \n",
    "        ax.set_xlabel('epoch')\n",
    "        ax.set_title(title)\n",
    "        \n",
    "    plt.tight_layout()                                                          \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Data\n",
    "-------------\n",
    "\n",
    "+ `circles`: a 2D, two-class problem where the decision boundary is a circle. \n",
    "+ `fashion mnist`: like mnist, but with clothes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use helper functions to load the data.\n",
    "circle_data = get_circles_data()\n",
    "make_mnist_proc('data/fashion_mnist.pkl')\n",
    "mnist_data = load_pickle('data/fashion_mnist.pkl')\n",
    "\n",
    "# Plot the circle data.\n",
    "X = circle_data['X']['train']\n",
    "y = circle_data['y']['train']\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.8, c=y, cmap='RdYlBu')\n",
    "plt.axis('off')\n",
    "plt.title('circle data')\n",
    "plt.show()\n",
    "\n",
    "# Plot some examples from Fashion MNIST.\n",
    "X = mnist_data['X']['train']\n",
    "y = mnist_data['y']['train']\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, figsize=(9, 9))\n",
    "for ax in axs.ravel():\n",
    "    \n",
    "    example = np.random.choice(range(len(X)))\n",
    "    x_sample = X[example, :]\n",
    "    y_sample = y[example]\n",
    "    \n",
    "    # Reshape the images to be a square, they're flat for our MLP.\n",
    "    x_sample = x_sample.reshape((28, 28))\n",
    "    \n",
    "    ax.imshow(x_sample)\n",
    "    ax.set_title('y={}'.format(y_sample))\n",
    "    \n",
    "fig.suptitle(\"fashion MNIST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Handling\n",
    "--------------------\n",
    "\n",
    "The data structure we're using is nested dictionaries with train, valid, and test splits for both X and y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = circle_data['X']['train'] \n",
    "y = circle_data['y']['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Forward Pass\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    \"\"\"\n",
    "    ReLU is the maximum of 0 and X, aka, a rectifier activation.\n",
    "    \"\"\"\n",
    "    return np.maximum(np.zeros(X.shape), X)\n",
    "\n",
    "def softmax(X):\n",
    "    \"\"\"\n",
    "    Numerically stable softmax (note the subtraction of the max).\n",
    "    \"\"\"\n",
    "    exps = np.exp(X - np.max(X, axis=1).reshape(-1, 1))\n",
    "    exps /= np.sum(exps, axis=1).reshape(-1, 1)\n",
    "    \n",
    "    return exps \n",
    "\n",
    "def forward(X, W, b, activation):\n",
    "    \"\"\"\n",
    "    Passes the data X through the weights W, applying the bias b, \n",
    "    and finally the activation function supplied.\n",
    "    \"\"\"\n",
    "    pre_activations = X.dot(W) + b\n",
    "    outputs = activation(pre_activations)\n",
    "    \n",
    "    return (pre_activations, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Loss\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLLLoss:\n",
    "    \"\"\"Calculates the negative log likelyhood loss.\"\"\"\n",
    "    def __init__(self, n_out):\n",
    "        self.onehot = OneHotEncoder(sparse=False, n_values=n_out)\n",
    "        \n",
    "    def __call__(self, y_hat, y):\n",
    "        \"\"\"\n",
    "        Negative log likelihood loss between the prediction y_hat and the\n",
    "        ground-truth labels y.\n",
    "        \"\"\"\n",
    "        y_one = self.onehot.fit_transform(y.reshape(-1,1))\n",
    "        prob = np.einsum('ky,ky->k', y_hat, y_one)\n",
    "        loss = -np.log(prob)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    \n",
    "def l2_reg_loss(W):\n",
    "    \"\"\"Calculates the L2 penalty to apply to the weights.\"\"\"\n",
    "    l2_loss = np.sum(np.square(W))\n",
    "    \n",
    "    return l2_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(y_hat):\n",
    "    \"\"\"\n",
    "    The outputs of a softmax are probabilities, we take the max \n",
    "    as our hard prediction.\n",
    "    \"\"\"\n",
    "    pred = np.argmax(y_hat.T, axis=0)\n",
    "    return pred\n",
    "\n",
    "\n",
    "def accuracy(pred, y):\n",
    "    \"\"\"\n",
    "    Percentage of responses that are correct.\n",
    "    \"\"\"\n",
    "    correct = np.sum(pred == y)\n",
    "    total = len(y)\n",
    "    acc = (correct / float(total)) * 100\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Backward Pass\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxBackward:\n",
    "    \"\"\"backprop of softmax\"\"\"\n",
    "    def __init__(self, n_out):\n",
    "        self.onehot = OneHotEncoder(sparse=False, n_values=n_out)\n",
    "        \n",
    "    def __call__(self, y_hat, y):\n",
    "        y_one = self.onehot.fit_transform(y.reshape(-1,1))\n",
    "        return y_hat - y_one\n",
    "\n",
    "\n",
    "def relu_backward(z):\n",
    "    \"\"\"if previous layer pre-activated vals were <= 0, also set z to 0\"\"\"\n",
    "    z[z <= 0] = 0\n",
    "    z[z > 0] = 1\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight Initialization\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_W(n_in, n_out):\n",
    "    \"\"\"initializes weight matrix using glorot initialization.\"\"\"\n",
    "    # Sample weights uniformly from [-1/sqrt(in), 1/np.sqrt(in)].\n",
    "    bound = 1/np.sqrt(n_in)\n",
    "    W = np.random.uniform(low=-bound, high=bound, size=(n_in, n_out))\n",
    "    return W\n",
    "\n",
    "\n",
    "def init_b(n_out):\n",
    "    \"\"\"Initializes a bias vector the size of the output layer.\"\"\"\n",
    "    b = np.zeros(n_out)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model\n",
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        hyperparameters = {\n",
    "            'n_in'  : 784,\n",
    "            'n_hid' : 10,\n",
    "            'n_out' : 10,\n",
    "            'lr'     : 0.01,\n",
    "            'l2'     : 0,\n",
    "        }\n",
    "\n",
    "        hyperparameters.update(kwargs)\n",
    "\n",
    "        self.n_in = hyperparameters['n_in']     # input dimensions\n",
    "        self.n_hid = hyperparameters['n_hid']   # number of hidden units\n",
    "        self.n_out = hyperparameters['n_out']   # number of output units\n",
    "        self.lr  = hyperparameters['lr']        # learning rate\n",
    "        self.l2 = hyperparameters['l2']         # lambda l1\n",
    "\n",
    "        # Construct network and initialize weights.\n",
    "        self.W1 = init_W(self.n_in, self.n_hid)\n",
    "        self.W2 = init_W(self.n_hid, self.n_out)\n",
    "        self.b1 = init_b(self.n_hid)\n",
    "        self.b2 = init_b(self.n_out)\n",
    "\n",
    "        # for loss calculation\n",
    "        self.negative_log_likelyhood = NLLLoss(self.n_out)\n",
    "        \n",
    "        # for backprop\n",
    "        self.softmax_backward = SoftmaxBackward(self.n_out)\n",
    "        \n",
    "    def pass_data(self, X, y, train=False):\n",
    "        \"\"\"take inputs, and push them through to produce a prediction y\"\"\"\n",
    "\n",
    "        batch_size = X.shape[0]\n",
    "        \n",
    "        hid_preact, hid = forward(X, self.W1, self.b1, relu)\n",
    "        out_preact, y_hat = forward(hid, self.W2, self.b2, softmax)\n",
    "\n",
    "        pred = predict(y_hat)\n",
    "        acc = accuracy(pred, y)\n",
    "        \n",
    "        # Calculating the loss.\n",
    "        loss = self.negative_log_likelyhood(y_hat, y)\n",
    "        loss += l2_reg_loss(self.W1) * self.l2\n",
    "        loss += l2_reg_loss(self.W2) * self.l2\n",
    "        \n",
    "        if train:\n",
    "\n",
    "            # Gradients for output --> hidden layer.\n",
    "            dloss_y_hat = self.softmax_backward(y_hat, y)\n",
    "            dloss_W2 = hid.T.dot(dloss_y_hat) / batch_size   \n",
    "            dloss_b2 = np.sum(dloss_y_hat, axis=0) / batch_size \n",
    "            \n",
    "            # Gradients for hidden --> input layer.\n",
    "            dloss_out_preact = self.W2.dot(dloss_y_hat.T)\n",
    "            dloss_hid = relu_backward(hid) * dloss_out_preact.T\n",
    "            dloss_W1  = X.T.dot(dloss_hid) / batch_size \n",
    "            dloss_b1  = np.sum(dloss_hid, axis=0) / batch_size\n",
    "            \n",
    "            assert dloss_W1.shape == self.W1.shape\n",
    "            assert dloss_W2.shape == self.W2.shape\n",
    "            assert dloss_b1.shape == self.b1.shape\n",
    "            assert dloss_b2.shape == self.b2.shape\n",
    "            \n",
    "            # Weight Decay Regularization.\n",
    "            reg_W1 = (self.l2 * 2 * self.W1)\n",
    "            reg_W2 = (self.l2 * 2 * self.W2)\n",
    "\n",
    "            # Gradient Descent!\n",
    "            self.W1 -= self.lr * (dloss_W1 + reg_W1)\n",
    "            self.W2 -= self.lr * (dloss_W2 + reg_W2)    \n",
    "            self.b1 -= self.lr * (dloss_b1)\n",
    "            self.b2 -= self.lr * (dloss_b2)\n",
    "\n",
    "        return (pred, loss, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training / Evaluation Loop\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, batch_size, X, y, train=False):\n",
    "    \n",
    "    minibatches = get_minibatches(X.shape[0], batch_size)\n",
    "\n",
    "    total_acc = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    for j, batch in enumerate(minibatches):\n",
    "        _, loss, acc = model.pass_data(X[batch, :], y[batch], train=train)\n",
    "        total_acc += np.mean(acc)\n",
    "        total_loss += np.mean(loss)\n",
    "\n",
    "    total_acc /= len(minibatches)\n",
    "    total_loss /= len(minibatches)\n",
    "\n",
    "    return (total_acc, total_loss)\n",
    "\n",
    "\n",
    "def train_model(model, data, epochs=100, batch_size=32, verbose=False):\n",
    "    \n",
    "    results = {'loss':{'train': [], 'valid' : [], 'test': []},\n",
    "               'accuracy': {'train': [], 'valid': [], 'test': []}}\n",
    "\n",
    "    # Unpack the data.\n",
    "    X_train = data['X']['train']\n",
    "    y_train = data['y']['train']\n",
    "    X_valid = data['X']['valid']\n",
    "    y_valid = data['y']['valid']\n",
    "    X_test = data['X']['test']\n",
    "    y_test = data['y']['test']\n",
    "        \n",
    "    for i in range(epochs):\n",
    "\n",
    "        train_acc, train_loss = run_epoch(model, batch_size, X_train, y_train, train=True)\n",
    "        if verbose:\n",
    "            print('TRAIN [epoch {}]: accuracy={:.4f}, loss={:.4f}'.format(i+1, train_acc, train_loss))\n",
    "\n",
    "        valid_acc, valid_loss = run_epoch(model, batch_size, X_valid, y_valid, train=False)\n",
    "        if verbose:\n",
    "            print('VALID [epoch {}]: accuracy={:.4f}, loss={:.4f}'.format(i+1, valid_acc, valid_loss))\n",
    "\n",
    "        test_acc, test_loss = run_epoch(model, batch_size, X_test, y_test, train=False)\n",
    "        if verbose:\n",
    "            print('TEST  [epoch {}]: accuracy={:.4f}, loss={:.4f}'.format(i+1, test_acc, test_loss))\n",
    "\n",
    "        # store end-of-epoch results for train, valid, test\n",
    "        results['loss']['train'].append(train_loss)\n",
    "        results['loss']['valid'].append(valid_loss)\n",
    "        results['loss']['test'].append(test_loss)\n",
    "        results['accuracy']['train'].append(train_acc)\n",
    "        results['accuracy']['valid'].append(valid_acc)\n",
    "        results['accuracy']['test'].append(test_acc)\n",
    "\n",
    "    return(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Train!\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Circles Data\n",
    "circles_model = MLP(n_in=2, n_hid=100, n_out=2, lr=0.1)\n",
    "circles_results = train_model(circles_model, circle_data, epochs=50, batch_size=32, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Data\n",
    "mnist_model = MLP(n_in=784, n_hid=100, n_out=10, lr=0.1)\n",
    "mnist_results = train_model(mnist_model, mnist_data, epochs=50, batch_size=32, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Boundary\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = circle_data['X']['valid']\n",
    "y_valid = circle_data['y']['valid']\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "model = MLP(n_in=2, n_hid=3, n_out=2, lr=0.1)\n",
    "_ = train_model(model, circle_data, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "plot_decision(model, X_valid, y_valid, title=\"Small HID Size\");\n",
    "\n",
    "model = MLP(n_in=2, n_hid=10, n_out=2, lr=0.1)\n",
    "_ = train_model(model, circle_data, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "plot_decision(model, X_valid, y_valid, title=\"Large HID Size\")\n",
    "\n",
    "model = MLP(n_in=2, n_hid=3, n_out=2, lr=0.1, l2=0.001)\n",
    "_ = train_model(model, circle_data, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "plot_decision(model, X_valid, y_valid, title=\"Small HID Size w/ L2 Regularization\");\n",
    "\n",
    "model = MLP(n_in=2, n_hid=10, n_out=2, lr=0.1, l2=0.001)\n",
    "_ = train_model(model, circle_data, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "plot_decision(model, X_valid, y_valid, title=\"Large HID Size w/ L2 Regularization\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Curves\n",
    "-----------------------\n",
    "\n",
    "+ Note the train, valid, and test losses over epochs for the two . Do you see any evidence of overfitting? Why do you think you do or do not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(circles_results, 'circles');\n",
    "plot_results(mnist_results, 'mnist')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
